{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import matplotlib as mpl\n",
    "importlib.reload(mpl); importlib.reload(plt); importlib.reload(sns)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = fetch_openml('adult', version=2, as_frame=True, return_X_y=True)\n",
    "df = df.drop(columns=['fnlwgt', 'education-num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, random_state=42\n",
    ")\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy of my classifier is \"\n",
    "      f\"{clf.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = y.value_counts()\n",
    "target_counts.plot(kind='barh', legend=True)\n",
    "_ = plt.title(f\"Class balance ratio \"\n",
    "              f\"{target_counts.min() / target_counts.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in df\n",
    "            if df[col].dtype.name != 'category']\n",
    "cat_cols = [col for col in df\n",
    "            if df[col].dtype.name == 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The numerical columns are\\n {num_cols}\")\n",
    "print(f\"The categorical columns are\\n {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='unknwon',\n",
    "                  add_indicator=True),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "num_preprocessor = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SimpleImputer(strategy='mean', add_indicator=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (cat_preprocessor, cat_cols),\n",
    "    (num_preprocessor, num_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=10000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(metric, clf, X, y_true, metric_params=None):\n",
    "    assert X.shape[0] == y_true.shape[0], \"Different samples size!!!\"\n",
    "    y_pred = clf.predict(X)\n",
    "    if metric_params is None:\n",
    "        metric_params = {}\n",
    "    score = metric(y_true, y_pred, **metric_params)\n",
    "    print(f\"The {metric.__name__.replace('_' , ' ')} \"\n",
    "          f\"is {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metric(accuracy_score, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metric(balanced_accuracy_score, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    columns=clf.classes_,\n",
    "    index=clf.classes_\n",
    ")\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(cm_df, annot=True, annot_kws={\"size\": 16},\n",
    "            cmap='Oranges',)\n",
    "\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for metric in (precision_score, recall_score, f1_score):\n",
    "    print_metric(metric, model, X_test, y_test,\n",
    "                 {'pos_label': '>50K'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in (precision_score, recall_score, f1_score):\n",
    "    print_metric(metric, model, X_test, y_test,\n",
    "                 {'pos_label': '<=50K'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "_ = plot_roc_curve(model[-1], model[0].transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = model.predict_proba(X_test)\n",
    "print(f\"The ROC-AUC score is \"\n",
    "      f\"{roc_auc_score(y_test, y_pred[:, 1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import sensitivity_score\n",
    "from imblearn.metrics import specificity_score\n",
    "\n",
    "for metric in (specificity_score, sensitivity_score):\n",
    "    print_metric(metric, model, X_test, y_test,\n",
    "                 {'pos_label': '>50K'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "print_metric(geometric_mean_score, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The influence of imbalanced dataset on machine-learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[np.flatnonzero(y == 1), 0],\n",
    "               X[np.flatnonzero(y == 1), 1],\n",
    "               color='yellow', alpha=0.8, edgecolor='k')\n",
    "    ax.scatter(X[np.flatnonzero(y == 0), 0],\n",
    "               X[np.flatnonzero(y == 0), 1],\n",
    "              color='indigo', alpha=0.8, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.5, 0.5],\n",
    "                           class_sep=1.2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")\n",
    "model = make_pipeline(StandardScaler(), LinearSVC(max_iter=10000))\n",
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 4))\n",
    "plot_decision_function(X, y, model, ax=ax[0])\n",
    "plot_decision_function(X_train, y_train, model, ax=ax[1])\n",
    "plot_decision_function(X_test, y_test, model, ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=2,\n",
    "                           n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.05, 0.95],\n",
    "                           class_sep=1.2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 4))\n",
    "plot_decision_function(X, y, model, ax=ax[0])\n",
    "plot_decision_function(X_train, y_train, model, ax=ax[1])\n",
    "plot_decision_function(X_test, y_test, model, ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the loss function for the logistic regression in the binary case:\n",
    "\n",
    "$L(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}\\left[ y^{(i)} \\log \\left( h_{\\theta} (x^{(i)}) \\right) + \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_{\\theta} (x^{(i)}) \\right) \\right]$\n",
    "\n",
    "We sum over the sample without applying any weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hellinger.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](proba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = fetch_openml('adult', version=2, as_frame=True, return_X_y=True)\n",
    "df = df.drop(columns=['fnlwgt', 'education-num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the different models\n",
    "def evaluate_classifier(clf, df_all_scores):\n",
    "    name = getattr(clf, 'name', clf.__class__.__name__)\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    balanced_score = balanced_accuracy_score(y_test, y_pred)\n",
    "    df_score = pd.DataFrame(\n",
    "        {name: [score, balanced_score]}, \n",
    "        index=['Test accuracy', 'Balanced accuracy']\n",
    "    )\n",
    "    df_all_scores = pd.concat([df_all_scores, df_score], axis=1).round(decimals=3)\n",
    "    return df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline classifier\n",
    "\n",
    "As we have seen before, we will compare the new classifiers with a dummy baseline which predict the most frequent label in the dataset. This baseline will highlight the improvement compared to naive strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.name = \"Most Frequent Classifier\"\n",
    "\n",
    "df_all_scores = pd.DataFrame()\n",
    "df_all_scores = evaluate_classifier(dummy_clf, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make use of the `class_weight` parameter\n",
    "\n",
    "A first class of methods rely on sample weights to correct the imbalance. The core idea here is to weight prediction mistakes on the minority class higher than mistakes on the most common class.\n",
    "\n",
    "##### In linear model\n",
    "\n",
    "In `scikit-learn`, some estimators have a `class_weight` parameter that permits to do this. The idea is that the ERM is changed such that\n",
    "$$\n",
    "    \\arg\\min_\\theta \\frac{1}{\\sum_i w_i} \\sum_i w_i 1\\{f_\\theta(X_i) = y_i\\}\n",
    "$$\n",
    "with weights $w_i = \\frac{n}{kn_i}$ with $n$ the total number of samples, $k$ the number of classes and $n_i$ the number of samples from class $y_i$. This effectively rebalance the training in learning both from positive and negative examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='unknwon',\n",
    "                  add_indicator=True),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "\n",
    "num_preprocessor = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SimpleImputer(strategy='mean', add_indicator=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_lr = make_column_transformer(\n",
    "    (cat_preprocessor, cat_cols),\n",
    "    (num_preprocessor, num_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lr = make_pipeline(\n",
    "    preprocessor_lr, LogisticRegression(max_iter=10000)\n",
    ")\n",
    "model_lr.name = 'Logistic Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_lr, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the `class_weight='balanced'` uses the values of `y` to automatically adjust weights inversely proportional to class frequencies in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "model_lr_balanced = clone(model_lr)\n",
    "model_lr_balanced.set_params(\n",
    "    logisticregression__class_weight='balanced')\n",
    "model_lr_balanced.name = \"Logistic Regression with balanced weights\"\n",
    "df_all_scores = evaluate_classifier(model_lr_balanced, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In tree-based model\n",
    "\n",
    "In tree based models, the `class_weight` option is used to chose on the splits. Indeed, the purity criterion (which is minimize for to chose the split) is computed using these weights. In the leaf, the weights are used to compute the class to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cat_preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='unknwon',\n",
    "                  add_indicator=True),\n",
    "    OrdinalEncoder()\n",
    ")\n",
    "\n",
    "num_preprocessor = SimpleImputer(strategy='mean', add_indicator=True)\n",
    "\n",
    "preprocessor_rf = make_column_transformer(\n",
    "    (cat_preprocessor, cat_cols),\n",
    "    (num_preprocessor, num_cols)\n",
    ")\n",
    "model_rf = make_pipeline(\n",
    "    preprocessor_rf,\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "model_rf.name = \"Random Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_rf, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_balanced = clone(model_rf)\n",
    "model_rf_balanced.set_params(\n",
    "    randomforestclassifier__class_weight='balanced')\n",
    "model_rf_balanced.name = \"Balanced Random Forest\"\n",
    "\n",
    "df_all_scores = evaluate_classifier(model_rf_balanced, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ensemble models, `class_weight` can also take value `balanced_subsample`. This option is equivalent to the `'balanced'` one except that the weigths are computed directly for the bootstrap sample of each tree instead of weights computed globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_subbalanced = clone(model_rf)\n",
    "model_rf_subbalanced.set_params(\n",
    "    randomforestclassifier__class_weight='balanced_subsample')\n",
    "model_rf_subbalanced.name = \"Balanced Subsample Random Forest\"\n",
    "\n",
    "df_all_scores = evaluate_classifier(model_rf_subbalanced, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Resample the training set to have balanced classes\n",
    "\n",
    "A second option to learn on unbalanced data is to reweight the classes by sampling a new training set with balanced class. This can be done by either subsampling, oversampling or more complicated scheme demonstrated in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random under-sampling during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_undersampled = make_pipeline_imblearn(\n",
    "    preprocessor_lr,\n",
    "    RandomUnderSampler(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    ")\n",
    "model_lr_undersampled.name = \"Logistic Regression from rebalanced undersampled data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_lr_undersampled, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random over-sampling during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_oversampled = make_pipeline_imblearn(\n",
    "    preprocessor_lr,\n",
    "    RandomOverSampler(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    ")\n",
    "model_lr_oversampled.name = \"Logistic Regression from rebalanced oversampled data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_lr_oversampled, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More fancy methods\n",
    "\n",
    "\n",
    "There exists some more fancy methods to re-balance the dataset. For instance the SMOTE method where extra points are generated by creatinig synthetic points for the minority class. See more info on the [original paper](https://arxiv.org/pdf/1106.1813.pdf) or in this [blog post](http://rikunert.com/SMOTE_explained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_smote = make_pipeline_imblearn(\n",
    "    preprocessor_lr,\n",
    "    SMOTE(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    ")\n",
    "model_lr_smote.name = \"Logistic Regression from SMOTE sampled data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_lr_smote, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look in [imbalanced-learn documentation](https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.over_sampling) for more sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Used balanced algorithms: `BalancedRandomForest` and `BalancedBaggingClassifier` \n",
    "\n",
    "Instead of just sampling the training set to rebalance the classes, it is also possible to used _balanced_ classifier to fit the unbalanced dataset. The core idea is to use ensemble techniques with specific boostrap sampling strategies that make sure that each bootstrap sample is balanced.\n",
    "\n",
    "#### Example of `BalancedRandomForestClassifier`\n",
    "\n",
    "Here, a random forest is learn on the full dataset. Each tree is constructed using a balanced sub-sampled of the dataset.  \n",
    "This idea has been proposed by [Chen et al. (2004)](https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "model_balanced_rf = make_pipeline(\n",
    "    preprocessor_rf,\n",
    "    BalancedRandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "model_balanced_rf.name = \"Balanced Random Forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_balanced_rf, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of `BalancedBaggingClassifier`\n",
    "\n",
    "In ensemble classifiers, bagging methods build several estimators on different randomly selected subset of data. In scikit-learn, this classifier is named `BaggingClassifier`. However, this classifier does not allow to balance each subset of data. Therefore, when training on imbalanced data set, this classifier will favor the majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bagging = make_pipeline(\n",
    "    preprocessor_rf,\n",
    "    BaggingClassifier(base_estimator=HistGradientBoostingClassifier(),\n",
    "                      n_estimators=10, random_state=42, n_jobs=-1)\n",
    ")\n",
    "model_bagging.name = \"Bagging Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_bagging, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BalancedBaggingClassifier `allows to resample each subset of data before to train each estimator of the ensemble. In short, it combines the output of an `EasyEnsemble` sampler with an ensemble of classifiers (i.e. `BaggingClassifier`). Therefore, `BalancedBaggingClassifier `takes the same parameters than the scikit-learn `BaggingClassifier`. Additionally, there is two additional parameters, sampling_strategy and replacement to control the behaviour of the random under-sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "model_balanced_bagging = make_pipeline(\n",
    "    preprocessor_rf,\n",
    "    BalancedBaggingClassifier(base_estimator=HistGradientBoostingClassifier(),\n",
    "                              n_estimators=10, random_state=42, n_jobs=-1)\n",
    ")\n",
    "model_balanced_bagging.name = \"Balanced Bagging Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_scores = evaluate_classifier(model_balanced_bagging, df_all_scores)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, OneClassSVM(kernel='linear', nu=0.1)\n",
    ")\n",
    "mask_single_class = y_train == '>50K'\n",
    "_ = model.fit(X_train[mask_single_class], y_train[mask_single_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_converted = (2 * (y_test == '>50K').astype(int)) - 1\n",
    "score = balanced_accuracy_score(y_test_converted, y_pred)\n",
    "print(f\"Balanced accuracy {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
